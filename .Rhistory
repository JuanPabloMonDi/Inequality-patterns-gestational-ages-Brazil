## Exercicio 3 ------------------------------------------------
library(ipred)
library(randomForest)
install.packages("randomForest")
library(randomForest)
help(bagging)
a<-bagging(
formula = Kyphosis~.,
data= kyphosis,
nbagg=5000,
coob=TRUE
)
a<-bagging(
formula = Kyphosis~.,
data= kyphosis,
nbagg=50,
coob=TRUE
)
a
a<-bagging(
formula = Kyphosis~.,
data= kyphosis,
nbagg=500,
coob=TRUE
)
a
a<-bagging(
formula = Kyphosis~.,
data= kyphosis,
nbagg=5000,
coob=TRUE
)
install.packages("adabag")
library(adabag)
help(bagging)
library(dplyr)
library(tidyr)
library(mlbench)
library(kernlab) #Para RVM
library(ggplot2) #Para gráficos
library(Matrix)
library(gam)
library(latex2exp)
library(rpart); library(rpart.plot) #Para arvores de decisao
library(e1071) #Para SVM
library(ipred) #Para bagging
library(gbm) #Para gradient boosting
library(randomForest) # Para florestas aleatorias
library(RColorBrewer)
library(rattle)
library(adabag)
library(caret)
data("PimaIndiansDiabetes", package = "mlbench")
data("PimaIndiansDiabetes2", package = "mlbench")
set.seed(131101)
# Carga los datos
df <- na.omit(PimaIndiansDiabetes)
fit_rvm_classification <- function(X, t, sigma = 1, tol = 1e-6, max_iter = 500) {
N <- nrow(X)
# Paso 1: Matriz Phi (kernel)
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
sigmoid <- function(x) 1 / (1 + exp(-x))
Phi <- rbf_kernel(X, X, sigma)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
M <- ncol(Phi)
alpha <- rep(1, M)
w <- rep(0, M)
for (iter in 1:max_iter) {
# Paso 2: IRLS - salida y matriz B
y <- as.vector(Phi %*% w)
y_prob <- sigmoid(y)
B <- diag(y_prob * (1 - y_prob) + 1e-8)  # evitar 0 en B
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
# Paso 4: nuevo w usando Newton-Raphson (modo del posterior)
z <- Phi %*% w + solve(B) %*% (t - y_prob)
mu <- Sigma %*% t(Phi) %*% B %*% z
# Paso 5: actualizar alpha
gamma <- 1 - alpha * diag(Sigma)
alpha_new <- gamma / (mu^2 + 1e-10)
# Convergencia
if (max(abs(alpha - alpha_new)) < tol) break
alpha <- alpha_new
w <- as.vector(mu)
}
keep <- which(alpha < 1e9)
list(
alpha = alpha[keep],
w = w[keep],
X_train = X,
relevance_indices = keep,
kernel_sigma = sigma,
predict = function(X_new) {
K <- rbf_kernel(X_new, X, sigma)
Phi_new <- cbind(1, K)[, keep, drop = FALSE]
sigmoid(Phi_new %*% w[keep])
}
)
}
# Reescala las variables y codifica la respuesta
X <- scale(df[, -ncol(df)])  # estandarizar las variables
t <- ifelse(df$diabetes == "pos", 1, 0)  # clase binaria
# Agrega un sesgo (intercepto)
X <- cbind(1, X)  # columna de unos para w0
rbf_kernel <- function(X1, X2, sigma = 1) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
K <- exp(-dists / (2 * sigma^2))
return(K)
}
N <- nrow(X)
Phi <- rbf_kernel(X, X, sigma = 1)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
alpha <- rep(1, ncol(Phi))  # hiperparámetros iniciales
w <- rep(0, ncol(Phi))      # pesos iniciales
sigmoid <- function(x) 1 / (1 + exp(-x))
fit_rvm_classification <- function(X, t, sigma = 1, tol = 1e-6, max_iter = 500) {
N <- nrow(X)
# Paso 1: Matriz Phi (kernel)
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
sigmoid <- function(x) 1 / (1 + exp(-x))
Phi <- rbf_kernel(X, X, sigma)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
M <- ncol(Phi)
alpha <- rep(1, M)
w <- rep(0, M)
for (iter in 1:max_iter) {
# Paso 2: IRLS - salida y matriz B
y <- as.vector(Phi %*% w)
y_prob <- sigmoid(y)
B <- diag(y_prob * (1 - y_prob) + 1e-8)  # evitar 0 en B
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
# Paso 4: nuevo w usando Newton-Raphson (modo del posterior)
z <- Phi %*% w + solve(B) %*% (t - y_prob)
mu <- Sigma %*% t(Phi) %*% B %*% z
# Paso 5: actualizar alpha
gamma <- 1 - alpha * diag(Sigma)
alpha_new <- gamma / (mu^2 + 1e-10)
# Convergencia
if (max(abs(alpha - alpha_new)) < tol) break
alpha <- alpha_new
w <- as.vector(mu)
}
keep <- which(alpha < 1e9)
list(
alpha = alpha[keep],
w = w[keep],
X_train = X,
relevance_indices = keep,
kernel_sigma = sigma,
predict = function(X_new) {
K <- rbf_kernel(X_new, X, sigma)
Phi_new <- cbind(1, K)[, keep, drop = FALSE]
sigmoid(Phi_new %*% w[keep])
}
)
}
X_all <- scale(df[, -ncol(df)])
t_all <- ifelse(df$diabetes == "pos", 1, 0)
train_idx <- sample(1:nrow(X_all), 0.7 * nrow(X_all))
X_train <- X_all[train_idx, ]
t_train <- t_all[train_idx]
X_test  <- X_all[-train_idx, ]
t_test  <- t_all[-train_idx]
rvm_model <- fit_rvm_classification(X_train, t_train, sigma = 10)
fit_rvm_classification <- function(X, t, sigma = 1, tol = 1e-6, max_iter = 500) {
N <- nrow(X)
# Paso 1: Matriz Phi (kernel)
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
sigmoid <- function(x) 1 / (1 + exp(-x))
Phi <- rbf_kernel(X, X, sigma)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
M <- ncol(Phi)
alpha <- rep(1, M)
w <- rep(0, M)
for (iter in 1:max_iter) {
# Paso 2: IRLS - salida y matriz B
y <- as.vector(Phi %*% w)
y_prob <- sigmoid(y)
B <- diag(y_prob * (1 - y_prob) + 1e-8)  # evitar 0 en B
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
#H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
# Paso 4: nuevo w usando Newton-Raphson (modo del posterior)
z <- Phi %*% w + solve(B) %*% (t - y_prob)
mu <- Sigma %*% t(Phi) %*% B %*% z
# Paso 5: actualizar alpha
gamma <- 1 - alpha * diag(Sigma)
alpha_new <- gamma / (mu^2 + 1e-10)
# Convergencia
if (max(abs(alpha - alpha_new)) < tol) break
alpha <- alpha_new
w <- as.vector(mu)
}
keep <- which(alpha < 1e9)
list(
alpha = alpha[keep],
w = w[keep],
X_train = X,
relevance_indices = keep,
kernel_sigma = sigma,
predict = function(X_new) {
K <- rbf_kernel(X_new, X, sigma)
Phi_new <- cbind(1, K)[, keep, drop = FALSE]
sigmoid(Phi_new %*% w[keep])
}
)
}
X_all <- scale(df[, -ncol(df)])
t_all <- ifelse(df$diabetes == "pos", 1, 0)
train_idx <- sample(1:nrow(X_all), 0.7 * nrow(X_all))
X_train <- X_all[train_idx, ]
t_train <- t_all[train_idx]
X_test  <- X_all[-train_idx, ]
t_test  <- t_all[-train_idx]
rvm_model <- fit_rvm_classification(X_train, t_train, sigma = 10)
# Reescala las variables y codifica la respuesta
X <- scale(df[, -ncol(df)])  # estandarizar las variables
t <- ifelse(df$diabetes == "pos", 1, 0)  # clase binaria
# Agrega un sesgo (intercepto)
X <- cbind(1, X)  # columna de unos para w0
rbf_kernel <- function(X1, X2, sigma = 1) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
K <- exp(-dists / (2 * sigma^2))
return(K)
}
N <- nrow(X)
Phi <- rbf_kernel(X, X, sigma = 1)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
768*768
768*768+768
alpha <- rep(1, ncol(Phi))  # hiperparámetros iniciales
w <- rep(0, ncol(Phi))      # pesos iniciales
sigmoid <- function(x) 1 / (1 + exp(-x))
sigma=1
tol=1e-8
max_iter=1000
N <- nrow(X)
# Definimos o Kernel a usar, no artículo do Tipping é usado o Kernel gaussiano
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
#Função de classificação para  regressão logística
sigmoid <- function(x){return(1 / (1 + exp(-x)))}
Phi <- rbf_kernel(X, X, sigma)
Phi <- rbf_kernel(X, X, sigma)
Phi <- cbind(1, Phi)
w <- rep(0, M)
M <- ncol(Phi)
w <- rep(0, ncol(Phi))
var_y <- sigmoid(y_x) #Usando la função sigmoide, estimamos sigma(y)
y_x <- as.vector(Phi %*% w)#Estimação da classe y. \hat{y}
iter=1
var_y <- sigmoid(y_x) #Usando la função sigmoide, estimamos sigma(y)
var_y * (1 - var_y)
B <- diag(var_y * (1 - var_y))  # evitar 0 en B
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
#H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
mu <- Sigma %*% t(Phi) %*% B %*% t_train
B
t_train
B %*% t_train
ncol*B
ncol(B)
length(y_x)
ncol(Phi)
N
# Carga los datos
df <- na.omit(PimaIndiansDiabetes)
# Reescala las variables y codifica la respuesta
X <- scale(df[, -ncol(df)])  # estandarizar las variables
t <- ifelse(df$diabetes == "pos", 1, 0)  # clase binaria
# Agrega un sesgo (intercepto)
X <- cbind(1, X)  # columna de unos para w0
# Definimos o Kernel a usar, no artículo do Tipping é usado o Kernel gaussiano
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
Phi <- rbf_kernel(X, X, sigma = 1)
N <- nrow(X)
alpha <- rep(1, ncol(Phi))  # hiperparámetros iniciales
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
w <- rep(0, ncol(Phi))      # pesos iniciales
alpha <- rep(1, ncol(Phi))
w <- rep(0, ncol(Phi))
y_x <- as.vector(Phi %*% w)#Estimação da classe y. \hat{y}
var_y <- sigmoid(y_x) #Usando la função sigmoide, estimamos sigma(y)
B <- diag(var_y * (1 - var_y))  #Matriz B para o calculo do Hessiano
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
#H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
# Paso 4: nuevo w usando Newton-Raphson (modo del posterior)
z <- Phi %*% w + solve(B) %*% (t - var_y)
t
train_idx <- sample(1:nrow(X_all), 0.7 * nrow(X_all))
X_train <- X_all[train_idx, ]
t_train <- t_all[train_idx]
X_test  <- X_all[-train_idx, ]
t_test  <- t_all[-train_idx]
# Agrega un sesgo (intercepto)
X_train <- cbind(1, X_train)  # columna de unos para w0
# Definimos o Kernel a usar, no artículo do Tipping é usado o Kernel gaussiano
rbf_kernel <- function(X1, X2, sigma) {
dists <- as.matrix(dist(rbind(X1, X2)))^2
n1 <- nrow(X1)
n2 <- nrow(X2)
dists <- dists[1:n1, (n1+1):(n1+n2)]
exp(-dists / (2 * sigma^2))
}
N <- nrow(X_train)
Phi <- rbf_kernel(X_train, X_train, sigma = 1)
Phi <- cbind(1, Phi)  # agregar sesgo explícitamente
alpha <- rep(1, ncol(Phi))  # hiperparámetros iniciales
w <- rep(0, ncol(Phi))      # pesos iniciales
N
#Função de classificação para  regressão logística
sigmoid <- function(x){return(1 / (1 + exp(-x)))}
Phi <- rbf_kernel(X_train, X_train, sigma)
Phi <- cbind(1, Phi)
M <- ncol(Phi)
alpha <- rep(1, ncol(Phi))
w <- rep(0, ncol(Phi))
y_x <- as.vector(Phi %*% w)#Estimação da classe y. \hat{y}
var_y <- sigmoid(y_x) #Usando la função sigmoide, estimamos sigma(y)
B <- diag(var_y * (1 - var_y))  #Matriz B para o calculo do Hessiano
# Paso 3: Hessiano y media posterior
A <- diag(alpha)
H <- t(Phi) %*% B %*% Phi + A
#H <- H + diag(1e-6, M)  # regularización
Sigma <- solve(H)
# Paso 4: nuevo w usando Newton-Raphson (modo del posterior)
z <- Phi %*% w + solve(B) %*% (t_train - var_y)
mu <- Sigma %*% t(Phi) %*% B %*% t_train
setwd("~/Unicamp/Outros trabalhos/Intergrowth PIG")
library(sf)
library(ggplot2)
library(geobr)
library(arrow); library(dplyr) ; library(geobr) ; library(stringr) ; library(roll) ; library(Kendall) ; library(trend) ; library(ggplot2) ; library(RColorBrewer) ; library(mltools) ; library(data.table) ; library(sf) ; library(ResourceSelection)
# Define el umbral (por ejemplo 30 grados)
umbral <- 0
detectar_exposicion_dias_info <- function(x, dias){
r <- (x > 0)  # Vector lógico de exposición
resultado <- rep(0, length(x))  # Para marcar los días donde empieza la secuencia
contador <- 0  # Cuenta cuántas veces ocurre
i<-1
while (i <= (length(x) - dias)) {
if (all(r[i:(i+2)])){
resultado[i] <- 1
contador <- contador + 1
i<- i+dias  # Salta al día después de la secuencia para evitar solapamientos
}else{
i<-i+1
}
}
# Clasificación final según cantidad de secuencias
clasificacion <- ifelse(contador == 0, "0",
ifelse(contador == 1, "1",
ifelse(contador == 2, "2", "3 ou mais")))  # 3 significa "3 o más"
return(list(clase = clasificacion, conteo = contador, dias_inicio = resultado,dias=dias))
}
detectar_si<-function(x,dias){
b<-detectar_exposicion_dias_info(x,dias)
b<-b$clase
return(b)
}
load("~/Unicamp/Outros trabalhos/Intergrowth PIG/Dados e RData/Tmax_Tmin_maximas_RH_Bulbos_municipal(nomes).RData")
contagem<-data.frame('CD_MUN'=bulbo_seco_municipal$CD_MUN)
ano=2016
janela_tempo<-function(n_dias, ano,
bulbo_seco_muni=bulbo_seco_municipal,
bulbo_umido_muni=bulbo_umido_municipal){
b <- bulbo_seco_muni[, c("CD_MUN", colnames(bulbo_seco_muni)[substr(colnames(bulbo_seco_muni), 1, 4) == ano])]
c <- bulbo_umido_muni[, c("CD_MUN", colnames(bulbo_umido_muni)[substr(colnames(bulbo_umido_muni), 1, 4) == ano])]
contagem<-data.frame('CD_MUN'=b$CD_MUN)
for (n in n_dias){
contagem_bulbo_seco <- apply(b[,-1],1,detectar_si,dias=n)
contagem_bulbo_umido <- apply(c[,-1],1,detectar_si,dias=n)
a <- data.frame("CD_MUN" = b$CD_MUN,
"seco" = contagem_bulbo_seco,
"umido" = contagem_bulbo_umido)
colnames(a)[2:3] <- c(paste0("seco", n), paste0("umido", n))
contagem<-merge(contagem,a,by="CD_MUN")
}
return(contagem)
}
contagem2016<-janela_tempo(n_dias = c(5),ano = "2016")
contagem2023<-janela_tempo(n_dias = c(5),ano = "2023")
estado_mapa<- read_state(code_state = "all", year = 2020, showProgress = F)
cidade_mapa <- read_municipality(code_muni = "all", year = 2020, showProgress = F)
colnames(contagem2016)[1]<-"code_muni"
colnames(contagem2023)[1]<-"code_muni"
cidade_mapa$code_muni<- as.numeric( substr(x = as.character(cidade_mapa$code_muni), start = 1, stop = 6) )
mapas2016<-merge(contagem2016, cidade_mapa, by="code_muni")
mapas2023<-merge(contagem2023, cidade_mapa, by="code_muni")
mapa2023 <- st_as_sf(mapas2023)
# Graficar
ggplot(data = mapa2016) +
geom_sf(aes(fill = seco3))+
scale_fill_brewer(palette = "Set3") +
theme_minimal()
# Asegúrate de que sea un objeto sf
mapa2016 <- st_as_sf(mapas2016)
mapa2023 <- st_as_sf(mapas2023)
# Graficar
ggplot(data = mapa2016) +
geom_sf(aes(fill = seco3))+
scale_fill_brewer(palette = "Set3") +
theme_minimal()
View(mapa2016)
contagem2016<-janela_tempo(n_dias = c(3),ano = "2016")
contagem2023<-janela_tempo(n_dias = c(3),ano = "2023")
colnames(contagem2016)[1]<-"code_muni"
colnames(contagem2023)[1]<-"code_muni"
mapas2016<-merge(contagem2016, cidade_mapa, by="code_muni")
mapas2023<-merge(contagem2023, cidade_mapa, by="code_muni")
# Asegúrate de que sea un objeto sf
mapa2016 <- st_as_sf(mapas2016)
mapa2023 <- st_as_sf(mapas2023)
# Graficar
ggplot(data = mapa2016) +
geom_sf(aes(fill = seco3))+
scale_fill_brewer(palette = "Set3") +
theme_minimal()
# Graficar
ggplot(data=mapa2016) +
geom_sf(aes(fill = seco3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
#scale_fill_brewer(type = "div", palette = 4, na.translate = FALSE) +
scale_fill_brewer(palette = "Set3") +
theme_minimal()
ggplot(data = mapa2023) +
geom_sf(aes(fill = seco3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
scale_fill_brewer(palette = "Set3") +
theme_minimal()
contagem2023<-janela_tempo(n_dias = c(3),ano = "2019")
colnames(contagem2023)[1]<-"code_muni"
mapas2023<-merge(contagem2023, cidade_mapa, by="code_muni")
mapa2023 <- st_as_sf(mapas2023)
ggplot(data = mapa2023) +
geom_sf(aes(fill = seco3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
scale_fill_brewer(palette = "Set3") +
theme_minimal()
ggplot(data=mapa2023) +
geom_sf(aes(fill = umido3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
#scale_fill_brewer(type = "div", palette = 4, na.translate = FALSE) +
scale_fill_brewer(palette = "Set3") +
theme_minimal()
ggplot(data=mapa2016) +
geom_sf(aes(fill = umido3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
#scale_fill_brewer(type = "div", palette = 4, na.translate = FALSE) +
scale_fill_brewer(palette = "Set3") +
theme_minimal()
contagem2023<-janela_tempo(n_dias = c(3),ano = "2023")
mapas2023<-merge(contagem2023, cidade_mapa, by="code_muni")
colnames(contagem2023)[1]<-"code_muni"
mapas2023<-merge(contagem2023, cidade_mapa, by="code_muni")
mapa2023 <- st_as_sf(mapas2023)
ggplot(data=mapa2023) +
geom_sf(aes(fill = umido3), color=NA) +  # municipalities colored
geom_sf(data = estado_mapa, fill = NA, color = "black", lwd = 0.3) +  # state outlines
#scale_fill_brewer(type = "div", palette = 4, na.translate = FALSE) +
scale_fill_brewer(palette = "Set3") +
theme_minimal()
